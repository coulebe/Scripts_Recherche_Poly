{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model discovery for PDE\n",
    "\n",
    "In this notebook we use the DeepMoD algorithm applied on the the PDE of an EWH. This version is just a test on the simulation datas. A final version will be developped when we have access to test bench again.\n",
    "\n",
    "We start by importing the required DeepMoD functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# DeepMoD functions\n",
    "\n",
    "\n",
    "from deepymod import DeepMoD\n",
    "from deepymod.data import Dataset, get_train_test_loader\n",
    "from deepymod.data.samples import Subsample_random\n",
    "from deepymod.model.func_approx import NN\n",
    "from deepymod.model.constraint import LeastSquares, GradParams, Ridge\n",
    "from deepymod.model.sparse_estimators import Threshold, PDEFIND\n",
    "from deepymod.training import train\n",
    "from deepymod.training.sparsity_scheduler import TrainTestPeriodic\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "from typing import Tuple\n",
    "from deepymod.utils.types import TensorList\n",
    "from deepymod import Library\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Configuring GPU or CPU\n",
    "if False: #torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we send the data into the Dataset format, create a plot to get an idea of the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load('EWH_sim_NullPower_CN_data.npy', allow_pickle=True).item()\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.contourf(data['x'], data['t']/3600, np.real(data['u']))\n",
    "# V_grid = torch.from_numpy(data[\"V\"])\n",
    "# coords_grid = torch.from_numpy(np.stack((data[\"t\"],data[\"x\"]), axis=-1)).float()\n",
    "ax.set_xlabel('x(m)')\n",
    "ax.set_ylabel('t(h)')\n",
    "fig.colorbar(mappable=im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that loads the data, makes torch tensors out of it and then returns it in a coords, data format. The shape of the samples will be (t,x) for the input and (u) for the dataset. Ensure that any array is not 1D, so an array with a single feature can be the shape (N,1) using reshape(-1,1) in numpy or unsqueeze(-1) in torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    array = np.load('EWH_sim_NullPower_CN_data.npy', allow_pickle=True).item()\n",
    "    coords = torch.from_numpy(np.stack((array[\"t\"],array[\"x\"], array[\"V\"]), axis=-1)).float()\n",
    "    data = torch.from_numpy(np.real(array[\"u\"])).unsqueeze(-1).float()\n",
    "    return coords, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass the function that loads the data to the DeePyMoD Dataset module, which loads the data, preprocesses it, subsamples it and then sends it to the right device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# V = torch.from_numpy(np.real(data[\"V\"])).unsqueeze(-1).float()\n",
    "\n",
    "dataset = Dataset(\n",
    "    load_data,\n",
    "    subsampler=Subsample_random,\n",
    "    subsampler_kwargs={\"number_of_samples\": 2500},\n",
    "     preprocess_kwargs={\n",
    "        \"normalize_coords\": False,\n",
    "        \"normalize_data\":  False,\n",
    "    },\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split our data into a train and test dataloaders for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_train_test_loader(dataset, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a custom library\n",
    "\n",
    "In this cell we'll create a custom build library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "from itertools import combinations, product\n",
    "from functools import reduce\n",
    "from deepymod.model.library import library_poly, library_deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%First of all, we need to defin a function that calculate the value of Q(t,x) for a given coords dataset \n",
    "# def Q_(data: torch.Tensor, Q_mat: torch.Tensor, coords: torch.Tensor):\n",
    "#     t, x = coords[0,:,0], coords[:,0,1]\n",
    "#     samples = data.shape[0]\n",
    "#     Q_value = torch.ones_like(data[:,0:1])\n",
    "#     for n in np.arange(samples):\n",
    "#         i = torch.argmin(torch.abs( x - data[n,1]))\n",
    "#         j = torch.argmin(torch.abs( t - data[n,0]))\n",
    "        \n",
    "#         Q_value[n] = Q_mat[i,j]\n",
    "        \n",
    "    \n",
    "#     return Q_value\n",
    "\n",
    "\n",
    "class Library_EWH(Library):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        Library ([type]): [description]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, poly_order: int = 1) -> None:\n",
    "        \"\"\"Calculates the temporal derivative a library/feature matrix consisting of\n",
    "        1) polynomials up to order poly_order, i.e. u, u^2...\n",
    "        2) derivatives up to order diff_order, i.e. u_x, u_xx\n",
    "        3) cross terms of 1) and 2), i.e. $uu_x$, $u^2u_xx$\n",
    "\n",
    "        Order of terms is derivative first, i.e. [$1, u_x, u, uu_x, u^2, ...$]\n",
    "\n",
    "        Only works for 1D+1 data. Also works for multiple outputs but in that case doesn't calculate\n",
    "        polynomial and derivative cross terms.\n",
    "\n",
    "        Args:\n",
    "            poly_order (int): maximum order of the polynomial in the library\n",
    "            diff_order (int): maximum order of the differentials in the library\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.poly_order = poly_order\n",
    "        # self.diff_order = diff_order\n",
    "\n",
    "    def library(\n",
    "        self, input: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[TensorList, TensorList]:\n",
    "        \"\"\"Compute a 2D library up to given polynomial order with second order derivatives\n",
    "         i.e. for poly_order=1: [$1, u_x, u_Q, u_{xx}, u_{QQ}, u_{xQ}, Q$] because is considered as an input of our neural network\n",
    "\n",
    "        Args:\n",
    "            input (Tuple[torch.Tensor, torch.Tensor]): A prediction u (n_samples, n_outputs) and spatiotemporal locations (n_samples, 3).(Q is the thrid input)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[TensorList, TensorList]: The time derivatives and the thetas\n",
    "            computed from the library and data.\n",
    "        \"\"\"\n",
    "        prediction, data = input\n",
    "        # Polynomial\n",
    "\n",
    "        u = torch.ones_like(prediction)\n",
    "        for order in np.arange(1, self.poly_order + 1):\n",
    "            u = torch.cat((u, u[:, order - 1 : order] * prediction), dim=1)\n",
    "\n",
    "        # Gradients\n",
    "        du = grad(\n",
    "            prediction,\n",
    "            data,\n",
    "            grad_outputs=torch.ones_like(prediction),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        u_t = du[:, 0:1]\n",
    "        u_x = du[:, 1:2]\n",
    "        # u_Q = du[:, 2:3]\n",
    "        du2 = grad(\n",
    "            u_x, data, grad_outputs=torch.ones_like(prediction), create_graph=True\n",
    "        )[0]\n",
    "        u_xx = du2[:, 1:2]\n",
    "        \n",
    "        #We add V and multiply it time x\n",
    "        V = data[:, 2:3]\n",
    "\n",
    "\n",
    "        theta = torch.cat((torch.ones_like(prediction), u_x,  prediction,  u_xx, u_x * V), dim=1)\n",
    "\n",
    "\n",
    "        # theta = torch.cat((theta, data[:, 2:3]), dim=1) #We add Q in the library, at the end of theta\n",
    "        return [u_t], [theta]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring DeepMoD\n",
    "\n",
    "Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(3, [50, 50, 50, 50], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the library function: We select the custom build library we created earlier\n",
    "* [$1, u, \\frac{\\partial u}{\\partial x}, \\frac{\\partial^{2} u}{\\partial x^{2}}, V \\frac{\\partial u}{\\partial x}$] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = Library_EWH(poly_order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-8, the algorithm is converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Threshold(0.5)\n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=50, patience=200, delta=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the sparsity estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = LeastSquares()#GradParams(n_params= 5, n_eqs= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate the model and select the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMoD(network, library, estimator, constraint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), betas=(0.99, 0.99), amsgrad=True, lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeepMoD\n",
    "\n",
    "We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2000x3 and 2x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Cours\\Recherche\\Scripts_Recherche_Poly\\Python\\MoD\\PDE_EWH V_arg_func.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_dataloader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     optimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     sparsity_scheduler,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/data/deepymod/coupled_new/\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     max_iterations\u001b[39m=\u001b[39;49m\u001b[39m100000\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     delta\u001b[39m=\u001b[39;49m\u001b[39m1e-8\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/Scripts_Recherche_Poly/Python/MoD/PDE_EWH%20V_arg_func.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\deepymod\\training\\training.py:53\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, sparsity_scheduler, split, exp_ID, log_dir, max_iterations, write_iterations, **convergence_kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m data_train, target_train \u001b[39m=\u001b[39m train_sample\n\u001b[0;32m     52\u001b[0m \u001b[39m# ================== Training Model ============================\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m prediction, time_derivs, thetas \u001b[39m=\u001b[39m model(data_train)\n\u001b[0;32m     54\u001b[0m batch_losses[\u001b[39m1\u001b[39m, :, batch_idx] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\n\u001b[0;32m     55\u001b[0m     (prediction \u001b[39m-\u001b[39m target_train) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     56\u001b[0m )  \u001b[39m# loss per output\u001b[39;00m\n\u001b[0;32m     57\u001b[0m batch_losses[\u001b[39m2\u001b[39m, :, batch_idx] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\n\u001b[0;32m     58\u001b[0m     [\n\u001b[0;32m     59\u001b[0m         torch\u001b[39m.\u001b[39mmean((dt \u001b[39m-\u001b[39m theta \u001b[39m@\u001b[39m coeff_vector) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     ]\n\u001b[0;32m     66\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\deepymod\\model\\deepmod.py:247\u001b[0m, in \u001b[0;36mDeepMoD.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: torch\u001b[39m.\u001b[39mTensor\n\u001b[0;32m    233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, TensorList, TensorList]:\n\u001b[0;32m    234\u001b[0m     \u001b[39m\"\"\"The forward pass approximates the data, builds the time derivative and feature matrices\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m    and applies the constraint.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     prediction, coordinates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc_approx(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    248\u001b[0m     time_derivs, thetas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlibrary((prediction, coordinates))\n\u001b[0;32m    249\u001b[0m     coeff_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstraint((time_derivs, thetas))\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\deepymod\\model\\func_approx.py:34\u001b[0m, in \u001b[0;36mNN.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m\"\"\"Forward pass through the network. Returns prediction and the differentiable input\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mso we can construct the library.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m    (torch.Tensor, torch.Tensor): prediction of size (n_samples, n_outputs) and coordinates of size (n_samples, n_inputs).\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m coordinates \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(coordinates), coordinates\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2000x3 and 2x50)"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    optimizer,\n",
    "    sparsity_scheduler,\n",
    "    log_dir=\"/data/deepymod/coupled_new/\",\n",
    "    max_iterations=100000,\n",
    "    delta=1e-8,\n",
    "    patience=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that DeepMoD has converged, it has found the following coefficients to not be zero: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([False, False, False, False, False])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sparsity_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it found the following coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimator_coeffs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the absolute value of the coefficients\n",
    "* [$1, u, \\frac{\\partial u}{\\partial x}, \\frac{\\partial^{2} u}{\\partial x^{2}}, V \\frac{\\partial u}{\\partial x}$] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3.8068e-10, 1.2352e-11, 3.4147e-03, 3.2402e-03, 3.9601e-08])]\n"
     ]
    }
   ],
   "source": [
    "print(model.library.norms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8669bef76cc3be34fb1786b3a73f1aaa89fe0e7e221f4cb9254d705447b9109a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
