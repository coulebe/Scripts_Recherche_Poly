{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Korteweg de Vries equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the KdV equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# DeepMoD functions\n",
    "\n",
    "from deepymod import DeepMoD\n",
    "from deepymod.data import Dataset, get_train_test_loader\n",
    "from deepymod.data.samples import Subsample_random\n",
    "from deepymod.model.func_approx import NN\n",
    "from deepymod.model.library import Library1D\n",
    "from deepymod.model.constraint import LeastSquares\n",
    "from deepymod.model.sparse_estimators import Threshold, PDEFIND\n",
    "from deepymod.training import train\n",
    "from deepymod.training.sparsity_scheduler import TrainTestPeriodic\n",
    "\n",
    "# Settings for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Configuring GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we send the data into the Dataset format, create a plot to get an idea of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.load('data/burgers.npy', allow_pickle=True).item()\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.contourf(data['x'], data['t'], np.real(data['u']))\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('t')\n",
    "fig.colorbar(mappable=im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that loads the data, makes torch tensors out of it and then returns it in a coords, data format. The shape of the samples will be (t,x) for the input and (u) for the dataset. Ensure that any array is not 1D, so an array with a single feature can be the shape (N,1) using reshape(-1,1) in numpy or unsqueeze(-1) in torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    array = np.load('data/kdv.npy', allow_pickle=True).item()\n",
    "    coords = torch.from_numpy(np.stack((array[\"t\"],array[\"x\"]), axis=-1)).float()\n",
    "    data = torch.from_numpy(np.real(array[\"u\"])).unsqueeze(-1).float()\n",
    "    return coords, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass the function that loads the data to the DeePyMoD Dataset module, which loads the data, preprocesses it, subsamples it and then sends it to the right device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\n",
    "    load_data,\n",
    "    subsampler=Subsample_random,\n",
    "    subsampler_kwargs={\"number_of_samples\": 2000},\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now also wish to split the data into a train and test split, specifically into loaders, which handle the logic of passing the samples to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = get_train_test_loader(dataset, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring DeepMoD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NN(2, [30, 30, 30, 30], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms:\n",
    "* [$1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx}$]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = Library1D(poly_order=2, diff_order=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Threshold(0.1) \n",
    "sparsity_scheduler = TrainTestPeriodic(periodicity=50, patience=10, delta=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration of the sparsity estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = LeastSquares() \n",
    "# Configuration of the sparsity scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the model and select the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMoD(network, library, estimator, constraint).to(device)\n",
    "\n",
    "# Defining optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.99, 0.99), amsgrad=True, lr=1e-3) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeepMoD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run DeepMoD using all the options we have set and the training data:\n",
    "* The directory where the tensorboard file is written (log_dir)\n",
    "* The ratio of train/test set used (split)\n",
    "* The maximum number of iterations performed (max_iterations)\n",
    "* The absolute change in L1 norm considered converged (delta)\n",
    "* The amount of epochs over which the absolute change in L1 norm is calculated (patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12600  MSE: 1.89e-06  Reg: 2.06e-06  L1: 3.02e+00 Algorithm converged. Writing model to disk.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, test_dataloader, optimizer,sparsity_scheduler, log_dir='runs/KDV/', split=0.8, max_iterations=100000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparsity masks provide the active and non-active terms in the PDE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([False, False, False,  True, False,  True, False, False, False, False,\n",
       "         False, False])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sparsity_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimatior_coeffs gives the magnitude of the active terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [-0.9680053],\n",
      "       [ 0.       ],\n",
      "       [-1.73612  ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ],\n",
      "       [ 0.       ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(model.estimator_coeffs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Cours\\Recherche\\DeePyMoD\\examples\\PDE_KdV.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Cours/Recherche/DeePyMoD/examples/PDE_KdV.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m library\u001b[39m.\u001b[39;49mforward(dataset)\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\deepymod\\model\\deepmod.py:182\u001b[0m, in \u001b[0;36mLibrary.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    171\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]\n\u001b[0;32m    172\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorList, TensorList]:\n\u001b[0;32m    173\u001b[0m     \u001b[39m\"\"\"Compute the library (time derivatives and thetas) from a given dataset. Also calculates the norms\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    of these, later used to calculate the normalized coefficients.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39m        Tuple[TensorList, TensorList]: Temporal derivative and libraries of size ([(n_samples, 1) x n_outputs]), [(n_samples, n_features)x n_outputs])\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     time_derivs, thetas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorms \u001b[39m=\u001b[39m [\n\u001b[0;32m    184\u001b[0m         (torch\u001b[39m.\u001b[39mnorm(time_deriv) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mnorm(theta, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m    185\u001b[0m         \u001b[39m.\u001b[39mdetach()\n\u001b[0;32m    186\u001b[0m         \u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m    187\u001b[0m         \u001b[39mfor\u001b[39;00m time_deriv, theta \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(time_derivs, thetas)\n\u001b[0;32m    188\u001b[0m     ]\n\u001b[0;32m    189\u001b[0m     \u001b[39mreturn\u001b[39;00m time_derivs, thetas\n",
      "File \u001b[1;32mc:\\Users\\alfre\\anaconda3\\envs\\py38\\lib\\site-packages\\deepymod\\model\\library.py:106\u001b[0m, in \u001b[0;36mLibrary1D.library\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlibrary\u001b[39m(\n\u001b[0;32m     94\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]\n\u001b[0;32m     95\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TensorList, TensorList]:\n\u001b[0;32m     96\u001b[0m     \u001b[39m\"\"\"Compute the temporal derivative and library for the given prediction at locations given by data.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m        Data should have t in first column, x in second.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m        computed from the library and data.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     prediction, data \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    107\u001b[0m     poly_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    108\u001b[0m     deriv_list \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "library.forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8669bef76cc3be34fb1786b3a73f1aaa89fe0e7e221f4cb9254d705447b9109a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
